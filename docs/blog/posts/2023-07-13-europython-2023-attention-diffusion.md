---
date: 2023-07-20
layout: post
categories:
 - tech
tags:
  - neural networks
  - attention
  - diffusion
  - GenAI
  - LLM
comments: true
---

# Neural network architectures of LLMs and Diffusion Models

Presentation discussing neural network architectures of Large Language Models and Diffusion Models at EuroPython 2023.

Slides for a [presentation](https://ep2023.europython.eu/session/understanding-neural-network-architectures-with-attention-and-diffusion/)
I gave the [EuroPython 2023](https://ep2023.europython.eu/) conference in Prague.

<script defer class="speakerdeck-embed" data-id="f4840978f8f0476b915428e014fade1c" data-ratio="1.7777777777777777" src="//speakerdeck.com/assets/embed.js"></script>

<!-- more -->

## Abstract

Neural networks have revolutionized AI, enabling machines to learn from data and make intelligent decisions. In this talk, we'll explore two popular architectures: Attention models and Diffusion models.

First up, we'll discuss Attention models and how they've contributed to the success of large language models like ChatGPT. We'll explore how the Attention mechanism helps GPT focus on specific parts of a text sequence and how this mechanism has been applied to different tasks in natural language processing.

Next, we'll dive into Diffusion models, a class of generative models that have shown remarkable performance in image synthesis. We'll explain how they work and their potential applications in the creative industry.

By the end of the talk, you'll have a better understanding of these cutting-edge neural network architectures.

## Talk recording

<iframe width="560" height="315" src="https://www.youtube.com/embed/Clh0nJRMvNs?si=MRlJWwT-tUdx-6nQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
